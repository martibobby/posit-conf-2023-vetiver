---
title: Monitor your model
subtitle: Deploy and maintain models with vetiver (R)
format: html
---

## Data for modeling & data for monitoring ü§ù
### Types of drift:
- Data Drift - changes with the input data (ie. there are more observations over time). If input distribution changes, then outputs will drift as well
- Concept Drift - relationship between the input data changes, then we trained out model incorrectly and need to retrain

If they added new risk levels (instead of low, medium, high to A-F) what would that drift be?

```{r}
library(arrow)

path1 <- here::here("data", "inspections.parquet")
inspections <- read_parquet(path1)

path2 <- here::here("data", "inspections_monitoring.parquet")
inspections_new <- read_parquet(path2)
```

Compare the modeling and monitoring datasets:

```{r}
library(ggplot2)
## The type of restaurants
combined <- bind_rows(
  inspections |> mutate(monitor = "Training/testing"),
  inspections_new |> mutate(monitor = "Monitoring")
) 

# bias to inspecting more high risk
# 

combined |> 
  count(monitor, facility_type, risk) |>
  group_by(monitor) |> 
  mutate(proportion = n / sum(n)) |>
  ggplot(aes(facility_type, proportion, fill = monitor)) +
  geom_col(alpha = 0.8, position = "dodge") +
  scale_y_continuous(labels = scales::percent) +
  labs(fill = NULL, y = "% of establishments", x = NULL)

```

## Monitor metrics over time ‚è≥

# do we monitor the statistical efficacy of our model?

```{r}
library(vetiver)
library(tidymodels)
#url <- "https://colorado.posit.co/rsc/chicago-inspections-rstats/predict"
url <- "https://spectral-bullfinch.d01bf.fleeting.rstd.io/rsconnect/inspection-rf-api/predict"
endpoint <- vetiver_endpoint(url)

augmentView <- augment(endpoint, new_data = inspections_new) 

metrics_df <-
  augment(endpoint, new_data = inspections_new) |>
  mutate(.pred_class = as.factor(.pred_class)) |> 
  vetiver_compute_metrics(
    inspection_date, 
    "week", 
    results, 
    .pred_class,
    metric_set = metric_set(accuracy, kap, sensitivity) #sentitivity is the same as recall
  )

metrics_df
```

```{r}
vetiver_plot_metrics(metrics_df)
```

Read about how you can use pins for versioning metrics: <https://rstudio.github.io/vetiver-r/reference/vetiver_pin_metrics.html>

## ML metrics ‚û°Ô∏è organizational outcomes

What proportion of failed inspections were predicted to fail?

```{r}
augment(endpoint, inspections_new) |> 
  mutate(.pred_class = as.factor(.pred_class)) |> 
  group_by(facility_type) |> 
  sensitivity(results, .pred_class)
# THIS LOOKS AWFUL!!!
```

```{r}
## make a visualization
bind_rows(
  inspections |> mutate(monitor = "Training/testing"),
  inspections_new |> mutate(monitor = "Monitoring")
) |> 
  group_by(monitor, 
           inspection_date = lubridate::floor_date(inspection_date, unit = "month")) |> 
  summarise(results = mean(results == "PASS")) |> 
  ggplot(aes(inspection_date, results, color = monitor)) +
  geom_line(alpha = 0.8, linewidth = 1.5) +
  scale_y_continuous(labels = scales::percent) +
  labs(y = "% of inspections that have a PASS result", 
       x = NULL, 
       color = NULL)
```

## Create a monitoring dashboard or report üìä

To wrap up, create a model monitoring dashboard or report and publish it to Connect. You have a few options:

- Open the `04-monitor-dashboard.Rmd`, make any changes you like (for example, different metrics or visualizations), knit it, and publish to Connect.
- Open a fresh copy of the dashboard template in RStudio by choosing "File" ‚û°Ô∏è "New File" ‚û°Ô∏è "R Markdown" ‚û°Ô∏è "From Template" ‚û°Ô∏è "Vetiver Dashboard". Update this template for _your_ model and publish to Connect.
